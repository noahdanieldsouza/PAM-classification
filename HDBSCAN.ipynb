{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMw6dIf/nDkVlqnYFJB2p6R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noahdanieldsouza/PAM-classification/blob/main/HDBSCAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "y_UN3VT2LCcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchaudio numpy\n",
        "!pip install git+https://github.com/google-research/perch-hoplite.git@782acd0e409eb27df51a695de4cb6608dae0db25"
      ],
      "metadata": {
        "id": "6XfjOBCOg29K",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ksXPgHxgxmE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import csv\n",
        "import gc\n",
        "import tempfile\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "from perch_hoplite.agile import colab_utils, embed, source_info\n",
        "from perch_hoplite.db import sqlite_usearch_impl\n",
        "from perch_hoplite.zoo import model_configs\n",
        "from perch_hoplite.agile.classifier import LinearClassifier\n",
        "\n",
        "# --- Paths ---\n",
        "db_path = '/content/drive/My Drive/classifier_training/DB'\n",
        "classifier_path = '/content/drive/MyDrive/full_labeled_fish/DB/agile_classifier_v2.pt'\n",
        "\n",
        "\n",
        "# --- Load DB + model ---\n",
        "db = sqlite_usearch_impl.SQLiteUsearchDB.create(db_path)\n",
        "db_model_config = db.get_metadata('model_config')\n",
        "model_class = model_configs.get_model_class(db_model_config['model_key'])\n",
        "embedding_model = model_class.from_config(db_model_config['model_config'])\n",
        "embedding_ids = db.get_embedding_ids()\n",
        "print(f\"âœ… loaded {len(embedding_ids)} embeddings\")\n",
        "id = embedding_ids[0]\n",
        "vector = db.get_embedding(id)\n",
        "print(f\"âœ… vector {vector}\")\n",
        "\n",
        "# --- Load classifier ---\n",
        "classifier = LinearClassifier.load(classifier_path)\n",
        "class_names = classifier.classes\n",
        "print(\"âœ… Loaded classifier with classes:\", class_names)\n",
        "\n",
        "train_set = []\n",
        "\n",
        "limit = 0;\n",
        "for emb_id in embedding_ids:\n",
        "    limit += 1\n",
        "    if limit > 30000:\n",
        "        break\n",
        "    print (f\"processing embedding: {emb_id}\")\n",
        "    vector = db.get_embedding(emb_id)\n",
        "    print(vector.shape)\n",
        "\n",
        "    train_set.append(vector)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(np.shape(train_set))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Suppose this is your list of vectors\n",
        "# train_set = [array([..1280..]), array([..1280..]), ...]\n",
        "\n",
        "X = np.array(train_set)\n",
        "print(X.shape)   # (21600, 1280)"
      ],
      "metadata": {
        "id": "sgQ755KpjLe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#pca = PCA(n_components=90)  # or '0.95' for 95% explained variance\n",
        "#X_pca = pca.fit_transform(X_scaled)\n"
      ],
      "metadata": {
        "id": "JcMT1J_zbu5P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hdbscan\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=20,       # avoid fragmenting noise\n",
        "    min_samples=10,            # enforces strong density\n",
        "    cluster_selection_method='eom',  # better for noisy data\n",
        "    prediction_data=True)\n",
        "labels = clusterer.fit_predict(X_umap)"
      ],
      "metadata": {
        "id": "uYy6pArl3TVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "scatter = plt.scatter(X_umap[:,0], X_umap[:,1], c=labels, cmap='Spectral', s=5)\n",
        "\n",
        "# Create legend manually\n",
        "unique_labels = np.unique(labels)\n",
        "print(len(unique_labels) )\n",
        "for cluster_id in unique_labels:\n",
        "    if cluster_id == -1:\n",
        "        label_name = \"Noise\"\n",
        "    else:\n",
        "        label_name = f\"Cluster {cluster_id}\"\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=[f\"Cluster {l}\" for l in unique_labels], bbox_to_anchor=(1.05, 1))\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "EJnMV2Yc4N_h",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cursor = db._get_cursor()"
      ],
      "metadata": {
        "id": "QzccRBX0-oXn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import librosa\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Define your base path for audio files\n",
        "base_path = \"/content/drive/MyDrive/mass_data/cut2\"\n",
        "base_path2 = \"/content/drive/MyDrive/mass_data/cut1\"\n",
        "base_path3 = \"/content/drive/MyDrive/mass_data/cut\"\n",
        "\n",
        "# Get all unique clusters (excluding noise if present)\n",
        "unique_labels = np.unique(labels)\n",
        "\n",
        "\n",
        "# Loop through each cluster\n",
        "for cluster_id in unique_labels:\n",
        "    cluster_indices = np.where(labels == cluster_id)[0]\n",
        "    print(f\"\\nðŸŽ¯ Cluster {cluster_id} â€” {len(cluster_indices)} embeddings\")\n",
        "\n",
        "    # Skip clusters with fewer than 2 samples\n",
        "    if len(cluster_indices) < 2:\n",
        "        print(\"Not enough samples to display (skipping)\")\n",
        "        continue\n",
        "\n",
        "    # Randomly choose 2 embeddings from this cluster\n",
        "    sample_indices = random.sample(list(cluster_indices), 2)\n",
        "\n",
        "    for idx in sample_indices:\n",
        "        embedding_id = int(embedding_ids[idx])\n",
        "\n",
        "        # Query database for metadata\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "            SELECT\n",
        "              he.id,\n",
        "              hs.source,\n",
        "              hs.dataset,\n",
        "              he.offsets\n",
        "            FROM hoplite_embeddings he\n",
        "            JOIN hoplite_sources hs ON he.source_idx = hs.id\n",
        "            WHERE he.id = ?;\n",
        "            \"\"\",\n",
        "            (embedding_id,),\n",
        "        )\n",
        "        row = cursor.fetchone()\n",
        "\n",
        "        if not row:\n",
        "            print(f\"âš ï¸ No entry found for embedding_id {embedding_id}\")\n",
        "            continue\n",
        "\n",
        "        _, source, dataset, _ = row\n",
        "        file_path = os.path.join(base_path, source)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            file_path = os.path.join(base_path2, source)\n",
        "            if not os.path.exists(file_path):\n",
        "              file_path = os.path.join(base_path3, source)\n",
        "              if not os.path.exists(file_path):\n",
        "                print(f\"âŒ File not found: {file_path}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\nâ–¶ï¸ Cluster {cluster_id}: {source}\")\n",
        "        y, sr = librosa.load(file_path, sr=None)\n",
        "        display(ipd.Audio(y, rate=sr))\n"
      ],
      "metadata": {
        "id": "3N1A4_vJf1ZQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(\n",
        "    {\n",
        "        \"umap\": reducer,       # your trained UMAP reducer\n",
        "        \"clusterer\": clusterer # your trained HDBSCAN clusterer\n",
        "    },\n",
        "    \"/content/drive/MyDrive/umap_clusterer_model.joblib\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "udGZJdyJ3XlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = joblib.load(\"/content/drive/MyDrive/umap_clusterer_model.joblib\")\n",
        "print(model.keys())\n"
      ],
      "metadata": {
        "id": "SnRH0sVk4PBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import librosa\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Define your base path for audio files\n",
        "base_path = \"/content/drive/MyDrive/mass_data/cut2\"\n",
        "base_path2 = \"/content/drive/MyDrive/mass_data/cut1\"\n",
        "base_path3 = \"/content/drive/MyDrive/mass_data/cut\"\n",
        "\n",
        "# Get all unique clusters (excluding noise if present)\n",
        "unique_labels = np.unique(labels)\n",
        "\n",
        "\n",
        "# Loop through each cluster\n",
        "for cluster_id in [lbl for lbl in unique_labels if lbl == -1]:\n",
        "    cluster_indices = np.where(labels == cluster_id)[0]\n",
        "    print(f\"\\nðŸŽ¯ Cluster {cluster_id} â€” {len(cluster_indices)} embeddings\")\n",
        "\n",
        "    # Skip clusters with fewer than 2 samples\n",
        "    if len(cluster_indices) < 2:\n",
        "        print(\"Not enough samples to display (skipping)\")\n",
        "        continue\n",
        "\n",
        "    # Randomly choose 2 embeddings from this cluster\n",
        "    sample_indices = cluster_indices\n",
        "\n",
        "    for idx in sample_indices:\n",
        "        embedding_id = int(embedding_ids[idx])\n",
        "\n",
        "        # Query database for metadata\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "            SELECT\n",
        "              he.id,\n",
        "              hs.source,\n",
        "              hs.dataset,\n",
        "              he.offsets\n",
        "            FROM hoplite_embeddings he\n",
        "            JOIN hoplite_sources hs ON he.source_idx = hs.id\n",
        "            WHERE he.id = ?;\n",
        "            \"\"\",\n",
        "            (embedding_id,),\n",
        "        )\n",
        "        row = cursor.fetchone()\n",
        "\n",
        "        if not row:\n",
        "            print(f\"âš ï¸ No entry found for embedding_id {embedding_id}\")\n",
        "            continue\n",
        "\n",
        "        _, source, dataset, _ = row\n",
        "        file_path = os.path.join(base_path, source)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            file_path = os.path.join(base_path2, source)\n",
        "            if not os.path.exists(file_path):\n",
        "              file_path = os.path.join(base_path3, source)\n",
        "              if not os.path.exists(file_path):\n",
        "                print(f\"âŒ File not found: {file_path}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\nâ–¶ï¸ Cluster {cluster_id}: {source}\")\n",
        "        y, sr = librosa.load(file_path, sr=None)\n",
        "        display(ipd.Audio(y, rate=sr))"
      ],
      "metadata": {
        "id": "hAdiFrerzLuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "for i in unique_labels:\n",
        "    if i == 1:  # skip cluster ID 1\n",
        "        continue\n",
        "\n",
        "    cluster_indices = np.where(labels == i)[0]\n",
        "    print(f\"Cluster {i} has {len(cluster_indices)} embeddings\")\n",
        "\n",
        "    # Take up to 5 random samples (handles small clusters safely)\n",
        "    n_samples = (len(cluster_indices))\n",
        "    sample_indices = random.sample(list(cluster_indices), n_samples)\n",
        "\n",
        "    for idx in sample_indices:\n",
        "        embedding_id = int(embedding_ids[idx])\n",
        "\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "            SELECT\n",
        "              he.id,\n",
        "              hs.source,\n",
        "              hs.dataset,\n",
        "              he.offsets\n",
        "            FROM hoplite_embeddings he\n",
        "            JOIN hoplite_sources hs ON he.source_idx = hs.id\n",
        "            WHERE he.id = ?;\n",
        "            \"\"\",\n",
        "            (embedding_id,),\n",
        "        )\n",
        "\n",
        "        row = cursor.fetchone()\n",
        "        if row:\n",
        "            _, source, dataset, offsets = row\n",
        "            # Only print rows where the filename contains \"chunk\"\n",
        "            if \"chunk\" in source.lower():\n",
        "                print(row)\n",
        "        else:\n",
        "            print(f\"No entry found for embedding_id {embedding_id}\")\n"
      ],
      "metadata": {
        "id": "0EouoG5BC7_C",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_id = 3\n",
        "mask = labels == cluster_id\n",
        "X_sub = X[mask]\n"
      ],
      "metadata": {
        "id": "pvrG98Xs9wmn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "\n",
        "reducer_sub = umap.UMAP(\n",
        "    n_neighbors=5,\n",
        "    min_dist=0.0,\n",
        "    n_components=10\n",
        ")\n",
        "X_sub_umap = reducer_sub.fit_transform(X_sub)\n",
        "print (np.shape(X_sub_umap))\n"
      ],
      "metadata": {
        "id": "zu-V516z_IG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hdbscan\n",
        "\n",
        "clusterer_sub = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=10,\n",
        "    min_samples=3,\n",
        "    cluster_selection_method='eom'\n",
        ")\n",
        "labels_sub = clusterer_sub.fit_predict(X_sub_umap)\n"
      ],
      "metadata": {
        "id": "_0QVFlRs_N3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "scatter = plt.scatter(X_sub_umap[:,0], X_sub_umap[:,1], c=labels_sub, cmap='Spectral', s=5)\n",
        "\n",
        "# Create legend manually\n",
        "unique_labels = np.unique(labels_sub)\n",
        "print(len(unique_labels) )\n",
        "for cluster_id in unique_labels:\n",
        "    if cluster_id == -1:\n",
        "        label_name = \"Noise\"\n",
        "    else:\n",
        "        label_name = f\"Cluster {cluster_id}\"\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=[f\"Cluster {l}\" for l in unique_labels], bbox_to_anchor=(1.05, 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VC6l3T2o_SMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import librosa\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Define your base path for audio files\n",
        "base_path = \"/content/drive/MyDrive/mass_data/cut2\"\n",
        "base_path2 = \"/content/drive/MyDrive/mass_data/cut1\"\n",
        "base_path3 = \"/content/drive/MyDrive/mass_data/cut\"\n",
        "\n",
        "# Get all unique clusters (excluding noise if present)\n",
        "unique_labels = np.unique(labels_sub)\n",
        "\n",
        "\n",
        "# Loop through each cluster\n",
        "for cluster_id in unique_labels:\n",
        "    cluster_indices = np.where(labels_sub == cluster_id)[0]\n",
        "    print(f\"\\nðŸŽ¯ Cluster {cluster_id} â€” {len(cluster_indices)} embeddings\")\n",
        "\n",
        "    # Skip clusters with fewer than 2 samples\n",
        "    if len(cluster_indices) < 2:\n",
        "        print(\"Not enough samples to display (skipping)\")\n",
        "        continue\n",
        "\n",
        "    # Randomly choose 2 embeddings from this cluster\n",
        "    sample_indices = cluster_indices\n",
        "\n",
        "    for idx in sample_indices:\n",
        "        embedding_id = int(embedding_ids[idx])\n",
        "\n",
        "        # Query database for metadata\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "            SELECT\n",
        "              he.id,\n",
        "              hs.source,\n",
        "              hs.dataset,\n",
        "              he.offsets\n",
        "            FROM hoplite_embeddings he\n",
        "            JOIN hoplite_sources hs ON he.source_idx = hs.id\n",
        "            WHERE he.id = ?;\n",
        "            \"\"\",\n",
        "            (embedding_id,),\n",
        "        )\n",
        "        row = cursor.fetchone()\n",
        "\n",
        "        if not row:\n",
        "            print(f\"âš ï¸ No entry found for embedding_id {embedding_id}\")\n",
        "            continue\n",
        "\n",
        "        _, source, dataset, _ = row\n",
        "        file_path = os.path.join(base_path, source)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            file_path = os.path.join(base_path2, source)\n",
        "            if not os.path.exists(file_path):\n",
        "              file_path = os.path.join(base_path3, source)\n",
        "              if not os.path.exists(file_path):\n",
        "                print(f\"âŒ File not found: {file_path}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\nâ–¶ï¸ Cluster {cluster_id}: {source}\")\n",
        "        y, sr = librosa.load(file_path, sr=None)\n",
        "        display(ipd.Audio(y, rate=sr))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7l_BTX55_zZi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}